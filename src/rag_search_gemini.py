# ============================================================
#  File: src/rag_search_gemini.py
# ============================================================
# [ëª¨ë“ˆ ê°œìš”]
#   - Google Gemini ì„ë² ë”©(text-embedding-004) + FAISS ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´
#       "ì‚¬ìš©ì ì§ˆì˜ â†’ ë²¡í„° ê²€ìƒ‰ â†’ í›„ë³´ ì²­í¬ ì¬ë­í‚¹" ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë“ˆ.
#
#   - ì´ ëª¨ë“ˆì€ ìˆœìˆ˜íˆ "ê²€ìƒ‰" ì—­í• ë§Œ ë‹´ë‹¹í•˜ê³ ,
#     ì‹¤ì œ ë‹µë³€ ìƒì„±ì€ rag_qa_service.pyì—ì„œ ì²˜ë¦¬í•œë‹¤.
#
# [ì—­í• ]
#   1) ì§ˆì˜ ì„ë² ë”©
#      - text-embedding-004, output_dimensionality = 768 (ê¸°ë³¸)
#   2) FAISS ê²€ìƒ‰
#      - IndexFlatIP + L2 ì •ê·œí™”ëœ ë²¡í„° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
#   3) ì¬ë­í‚¹
#      - í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ìš°ì„ (ê°€ì¤‘ì¹˜ 1.2)
#      - ì§ˆì˜ í‚¤ì›Œë“œê°€ ì˜ ë§¤ì¹­ë˜ëŠ” ì²­í¬ì— ì¶”ê°€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
#      - ğŸ”¸ "ì™¸í˜•/í¬ê¸°/ì‚¬ì–‘" ê´€ë ¨ ì§ˆì˜ ì‹œ:
#          Â· 'ì œí’ˆ ì‚¬ì–‘/ê·œê²©/ì œì›' ì„¹ì…˜ ê°€ë²¼ìš´ ì¶”ê°€ ë¶€ìŠ¤íŒ…
#          Â· 'ê° ë¶€ì˜ ì´ë¦„/êµ¬ì„±í’ˆ/ì™¸í˜•' ì„¹ì…˜ ë° figure ì²­í¬ ë¶€ìŠ¤íŒ…
#          Â· ë°˜ëŒ€ë¡œ ì†Œë¹„ì í”¼í•´ë³´ìƒ/ë³´ì¦ì„œ/AS ì•ˆë‚´ ë“±ì€ ì†Œí­ ê°ì 
#   4) ğŸ”¹ ì œí’ˆ/ëª¨ë¸/ë„ë©´ ì½”ë“œ ì¸ë±ìŠ¤
#      - vectors_meta.jsonl ì „ì²´ë¥¼ í›‘ì–´ì„œ
#        "SBDH-T1000", "SAH001" ê°™ì€ ì½”ë“œ â†’ doc_id ë¦¬ìŠ¤íŠ¸ ë§¤í•‘ì„ ìƒì„±
#      - search() ì—ì„œ ì§ˆì˜ì— ì½”ë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´,
#        doc_id_filterê°€ ë¹„ì–´ ìˆì„ ë•Œ ìë™ìœ¼ë¡œ í•´ë‹¹ doc_idë¡œ í•„í„°ë§
#
#      - íŠ¹íˆ "ê°€ì¥ êµ¬ì²´ì ì¸ ì½”ë“œ"ë¥¼ ìš°ì„  ì‚¬ìš©:
#          ì˜ˆ) ì§ˆì˜: "SVC-WN2200MR í¬ê¸°ê°€ ì–¼ë§ˆë‚˜ ë¼?"
#              Â· ì¶”ì¶œ ì½”ë“œ: ["SVC-WN2200MR", "SVC", "WN2200MR"]
#              Â· "SVC-WN2200MR" / "WN2200MR" ê°™ì´ ìˆ«ìê°€ í¬í•¨ëœ ë” ê¸´ ì½”ë“œë¥¼
#                ìš°ì„ ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬, í•´ë‹¹ ëª¨ë¸ì— ê°€ì¥ ë°€ì ‘í•œ doc_idë§Œ ë‚¨ê¹€
#
# [ì…ë ¥ íŒŒì¼]
#   - data/index/faiss.index
#   - data/index/vectors_meta.jsonl
#
# [ì¶œë ¥]
#   - ì—†ìŒ (ê²€ìƒ‰ ê²°ê³¼ SearchResult ê°ì²´ ë°˜í™˜)
#
# [ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì£¼ìš” API]
#   - RagSearcher
#       searcher = RagSearcher()
#       result = searcher.search(
#           query="ì§ˆë¬¸ í…ìŠ¤íŠ¸",
#           top_k=8,
#           chunk_type_filter=None,      # "text" | "figure" | None
#           doc_id_filter=None,          # ["SAH001", "SBDH-T1000"] | None
#       )
#
# ============================================================

from __future__ import annotations

import json
import logging
import os
import re
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Sequence

import faiss  # type: ignore
import numpy as np
from dotenv import load_dotenv
from google import genai
from google.genai import types

# ----------------------------- ë¡œê±° / ê²½ë¡œ / ìƒìˆ˜ ì •ì˜ -----------------------------

logger = logging.getLogger(__name__)

PROJECT_ROOT: Path = Path(__file__).resolve().parents[1]

INDEX_ROOT_DIR: Path = PROJECT_ROOT / "data" / "index"
FAISS_INDEX_PATH: Path = INDEX_ROOT_DIR / "faiss.index"
VECTORS_META_PATH: Path = INDEX_ROOT_DIR / "vectors_meta.jsonl"

DEFAULT_EMBED_MODEL: str = "text-embedding-004"
DEFAULT_OUTPUT_DIM: int = 768

# ì¬ê²€ìƒ‰/ì¬ë­í‚¹ ê´€ë ¨ ìƒìˆ˜
DEFAULT_TOP_K: int = 8
DEFAULT_PRESEARCH_FACTOR: int = 3  # top_k * ì´ ê°’ ë§Œí¼ ë¨¼ì € FAISSì—ì„œ ë½‘ê¸°

TEXT_TYPE_BOOST: float = 1.2       # text ì²­í¬ ê°€ì¤‘ì¹˜
FIGURE_TYPE_BOOST: float = 1.0     # figure ì²­í¬ ê°€ì¤‘ì¹˜

KEYWORD_BOOST_PER_HIT: float = 0.1  # í‚¤ì›Œë“œ í•œ ë²ˆ ë§¤ì¹­ë  ë•Œë§ˆë‹¤ +0.1 ë°°
KEYWORD_MAX_HITS: int = 3           # ìµœëŒ€ 3íšŒê¹Œì§€ë§Œ ë°˜ì˜ (â†’ ìµœëŒ€ +0.3)


# ----------------------------- ë°ì´í„° êµ¬ì¡° ì •ì˜ -----------------------------


@dataclass
class RetrievedChunk:
    """
    ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë°˜í™˜í•  ë‹¨ì¼ ì²­í¬ ë‹¨ìœ„.

    - meta: vectors_meta.jsonl í•œ ì¤„ì— í•´ë‹¹í•˜ëŠ” ë©”íƒ€ë°ì´í„°
    """

    uid: str
    score: float              # ì¬ë­í‚¹ëœ ìµœì¢… ì ìˆ˜
    raw_score: float          # ìˆœìˆ˜ FAISS ìŠ¤ì½”ì–´ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    doc_id: str
    chunk_type: str           # "text" | "figure" | ê¸°íƒ€
    text: str
    meta: Dict[str, Any]


@dataclass
class SearchResult:
    """
    ê²€ìƒ‰ ê²°ê³¼ ì „ì²´ í‘œí˜„.
    """

    query: str
    top_k: int
    total_candidates: int            # ì¬ë­í‚¹ ëŒ€ìƒì´ ëœ í›„ë³´ ìˆ˜
    chunks: List[RetrievedChunk]


# ----------------------------- ê³µí†µ ìœ í‹¸ -----------------------------


def configure_logging() -> None:
    """
    ê°„ë‹¨í•œ ë¡œê·¸ ì„¤ì • (ìŠ¤í¬ë¦½íŠ¸ ë‹¨ë… ì‹¤í–‰ ì‹œ ì‚¬ìš©).
    """
    logging.basicConfig(
        level=logging.INFO,
        format="[%(levelname)s] %(message)s",
    )


def load_gemini_client() -> genai.Client:
    """
    Google Gemini í´ë¼ì´ì–¸íŠ¸ ìƒì„±.
    """
    load_dotenv()
    api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise RuntimeError("GEMINI_API_KEY (ë˜ëŠ” GOOGLE_API_KEY)ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    client = genai.Client(api_key=api_key)
    return client


def extract_vectors_from_response(resp: Any) -> List[List[float]]:
    """
    embed_content ì‘ë‹µì—ì„œ ë²¡í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ.
    rag_embedder_gemini.py ì™€ ë™ì¼í•œ ë¡œì§.
    """
    # batch ì‘ë‹µ í˜•íƒœ
    if hasattr(resp, "embeddings") and resp.embeddings is not None:
        vectors: List[List[float]] = []
        for emb in resp.embeddings:
            values = getattr(emb, "values", None)
            if values is None and isinstance(emb, dict):
                values = emb.get("values")
            if values is None:
                raise RuntimeError("ì„ë² ë”© ì‘ë‹µì—ì„œ values í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            vectors.append(list(values))
        return vectors

    # ë‹¨ì¼ ì‘ë‹µ í˜•íƒœ
    if hasattr(resp, "embedding") and resp.embedding is not None:
        values = getattr(resp.embedding, "values", None)
        if values is None and isinstance(resp.embedding, dict):
            values = resp.embedding.get("values")
        if values is None:
            raise RuntimeError("ì„ë² ë”© ì‘ë‹µì—ì„œ values í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return [list(values)]

    raise RuntimeError("embed_content ì‘ë‹µ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")


def normalize_vector(vec: np.ndarray) -> np.ndarray:
    """
    (N, D) ë˜ëŠ” (1, D) ë²¡í„°ë¥¼ L2 ì •ê·œí™”.
    """
    faiss.normalize_L2(vec)
    return vec


# ----------------------------- í‚¤ì›Œë“œ ì¶”ì¶œ/ë¶€ìŠ¤íŒ… -----------------------------


_KO_STOPWORDS = {
    "ëŠ”", "ì€", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ìœ¼ë¡œ",
    "ìœ¼ë¡œì¨", "ìœ¼ë¡œì„œ", "ê³¼", "ì™€", "ë„", "ë§Œ", "ë³´ë‹¤", "ë³´ë‹¤ë„",
    "ë•Œë¬¸ì—", "í•´ì„œ", "í•˜ì—¬", "í•˜ê³ ", "ì´ë©°", "ì…ë‹ˆë‹¤", "ì¸ê°€ìš”",
}

_EN_STOPWORDS = {
    "the", "is", "are", "and", "or", "of", "to", "in", "on",
    "for", "a", "an", "what", "how", "why", "who", "where",
}


def extract_keywords(query: str) -> List[str]:
    """
    ë§¤ìš° ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ì¶”ì¶œ:

    - ì†Œë¬¸ì ë³€í™˜
    - ì•ŒíŒŒë²³/ìˆ«ì/í•œê¸€ ì™¸ ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
    - ê¸¸ì´ 2 ë¯¸ë§Œ í† í°, ë¶ˆìš©ì–´(stopword)ëŠ” ì œê±°
    """
    q = query.strip().lower()
    if not q:
        return []

    # í•œê¸€/ì˜ë¬¸/ìˆ«ì ì™¸ì˜ ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
    q = re.sub(r"[^0-9a-zê°€-í£]+", " ", q)
    tokens = [t for t in q.split() if len(t) >= 2]

    keywords: List[str] = []
    for t in tokens:
        if t in _EN_STOPWORDS:
            continue
        if t in _KO_STOPWORDS:
            continue
        keywords.append(t)

    return keywords


def compute_reranked_score(
    base_score: float,
    meta: Dict[str, Any],
    keywords: List[str],
) -> Tuple[float, float, float]:
    """
    FAISS ê¸°ë³¸ ì ìˆ˜(base_score)ì—
      - í…ìŠ¤íŠ¸/figure íƒ€ì… ê°€ì¤‘ì¹˜
      - í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…
      - (ì§ˆì˜ ì˜ë„ ê¸°ë°˜) ì„¹ì…˜/figure ë¶€ìŠ¤íŒ…
    ì„ ê³±í•˜ì—¬ ìµœì¢… ì ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤.

    Returns:
        (final_score, type_boost, keyword_boost)
        - final_score ì•ˆì—ëŠ” ì„¹ì…˜/ì˜ë„ ë¶€ìŠ¤íŒ…ì´ ëª¨ë‘ ë°˜ì˜ëœ ê°’
    """
    chunk_type = (meta.get("chunk_type") or "").lower()
    if chunk_type == "text":
        type_boost = TEXT_TYPE_BOOST
    elif chunk_type == "figure":
        type_boost = FIGURE_TYPE_BOOST
    else:
        type_boost = 1.0

    # ---------------- í‚¤ì›Œë“œ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ----------------
    keyword_boost = 1.0
    if keywords:
        haystack = " ".join(
            str(meta.get(k, "")) for k in ("text", "doc_id", "uid")
        ).lower()
        hits = 0
        for kw in keywords:
            if kw and kw in haystack:
                hits += 1
            if hits >= KEYWORD_MAX_HITS:
                break
        if hits > 0:
            keyword_boost += KEYWORD_BOOST_PER_HIT * hits

    # ---------------- ì§ˆì˜ ì˜ë„ / ì„¹ì…˜ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ----------------
    #  - "í¬ê¸°/ì‚¬ì´ì¦ˆ/ê¸¸ì´/í­/ë†’ì´/ë¬´ê²Œ/ì‚¬ì–‘/spec" â†’ ì‚¬ì–‘/ê·œê²©/ì œì› ì„¹ì…˜ ìš°ì„ 
    #  - "ì–´ë–»ê²Œ ìƒê²¼/ìƒê¹€ìƒˆ/ëª¨ì–‘/ì™¸í˜•"          â†’ êµ¬ì„±/ê°ë¶€ ëª…ì¹­/ì™¸í˜• ì„¹ì…˜ + figure ìš°ì„ 
    section_boost = 1.0

    if keywords:
        kw_set = set(keywords)

        # í¬ê¸°/ì‚¬ì–‘ ì˜ë„ ê°ì§€
        size_keywords = {
            "í¬ê¸°", "ì‚¬ì´ì¦ˆ", "size", "dimensions",
            "ê¸¸ì´", "í­", "ë†’ì´", "ê°€ë¡œ", "ì„¸ë¡œ", "ë¬´ê²Œ", "ì¤‘ëŸ‰",
        }
        spec_keywords = {
            "ì‚¬ì–‘", "ìŠ¤í™", "spec", "specs", "specification", "ì œì›", "ê·œê²©",
        }
        appearance_keywords = {
            "ìƒê¹€ìƒˆ", "ëª¨ì–‘", "ì™¸í˜•", "appearance", "look", "looks",
        }

        is_size_or_spec_query = bool(kw_set & (size_keywords | spec_keywords)) or any(
            ("í¬ê¸°" in kw or "ì‚¬ì´ì¦ˆ" in kw or "dimensions" in kw)
            for kw in kw_set
        )
        is_appearance_query = bool(kw_set & appearance_keywords) or any(
            ("ìƒê²¼" in kw or "ìƒê¸´" in kw)
            for kw in kw_set
        )

        section_title = str(
            meta.get("section_title")
            or meta.get("category")
            or ""
        ).lower()

        if section_title:
            st = section_title

            # 1) ì‚¬ì–‘/ê·œê²©/ì œì› ì„¹ì…˜ ë¶€ìŠ¤íŒ… (í¬ê¸°/ì‚¬ì–‘ ê´€ë ¨ ì§ˆë¬¸ì¼ ë•Œ)
            if is_size_or_spec_query and any(
                hint in st
                for hint in ("ì‚¬ì–‘", "ê·œê²©", "ì œì›", "spec", "spec.", "specification")
            ):
                section_boost *= 1.15

            # 2) êµ¬ì„±/ê° ë¶€ ëª…ì¹­/ì™¸í˜• ì„¹ì…˜ ë¶€ìŠ¤íŒ… (ì™¸í˜•/ëª¨ì–‘ ì§ˆë¬¸ì¼ ë•Œ)
            if is_appearance_query and any(
                hint in st
                for hint in ("ê° ë¶€", "ê°ë¶€", "êµ¬ì„±", "êµ¬ì„±í’ˆ", "ì™¸ê´€", "ì™¸í˜•", "ëª…ì¹­")
            ):
                section_boost *= 1.15

            # 3) ì†Œë¹„ì í”¼í•´ë³´ìƒ / ë³´ì¦ì„œ / ì„œë¹„ìŠ¤ ì•ˆë‚´ëŠ”
            #    ì™¸í˜•/í¬ê¸°/ì‚¬ì–‘ ì§ˆë¬¸ì—ì„œëŠ” ì†Œí­ ê°ì 
            if (is_size_or_spec_query or is_appearance_query) and any(
                bad in st
                for bad in ("í”¼í•´ë³´ìƒ", "ì†Œë¹„ì", "ë³´ì¦ì„œ", "í’ˆì§ˆ ë³´ì¦", "ì„œë¹„ìŠ¤", "íê°€ì „", "ì¬í™œìš©")
            ):
                section_boost *= 0.85

        # 4) ì™¸í˜•/ëª¨ì–‘ ì§ˆë¬¸ì´ë©´ figure íƒ€ì…ì— ì¶”ê°€ ë¶€ìŠ¤íŒ…
        if is_appearance_query and chunk_type == "figure":
            section_boost *= 1.10

    final_score = base_score * type_boost * keyword_boost * section_boost
    return final_score, type_boost, keyword_boost


# ----------------------------- RagSearcher êµ¬í˜„ -----------------------------


class RagSearcher:
    """
    ë²¡í„° ì¸ë±ìŠ¤(FAISS) + ë©”íƒ€ ì •ë³´ë¥¼ ì‚¬ìš©í•´
    ì„¤ëª…ì„œ ì²­í¬ë¥¼ ê²€ìƒ‰í•˜ëŠ” ê²€ìƒ‰ê¸°.

    ğŸ”¹ ì¶”ê°€ ê¸°ëŠ¥:
      - vectors_meta.jsonl ì „ì²´ë¥¼ í›‘ì–´ì„œ
        'SBDH-T1000', 'SAH001' ê°™ì€ ì½”ë“œ â†’ doc_id ë§¤í•‘ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•.
      - search() í˜¸ì¶œ ì‹œ doc_id_filterê°€ ë¹„ì–´ ìˆê³ ,
        ì§ˆì˜ì—ì„œ ì½”ë“œê°€ ê°ì§€ë˜ë©´ ìë™ìœ¼ë¡œ í•´ë‹¹ doc_idë¡œ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ì¢íŒë‹¤.
      - ì½”ë“œê°€ ì—¬ëŸ¬ ê°œ ì„ì—¬ ìˆì„ ë•ŒëŠ” "ìˆ«ìë¥¼ í¬í•¨í•œ, ë” ê¸´ ì½”ë“œ"ë¥¼
        ìš°ì„ ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ëª¨ë¸ ë¬¸ì„œì—ë§Œ ë§¤í•‘í•œë‹¤.
      - doc_id_filterê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´, ê·¸ ë¬¸ì„œì˜ ë²¡í„°ë“¤ë§Œ ëŒ€ìƒìœ¼ë¡œ
        "ë¬¸ì„œ ë‚´ë¶€ ê²€ìƒ‰"ì„ ìˆ˜í–‰í•œë‹¤.
    """

    # ì œí’ˆ/ëª¨ë¸/ë„ë©´ ì½”ë“œ íŒ¨í„´ (ì˜ˆ: SBDH-T1000, SAH001 ë“±)
    #  - MODEL_CODE_RE : ëŒ€ë¬¸ì/ìˆ«ì 2~5 + '-' + 2~10 (ex. SBDH-T1000)
    #  - SIMPLE_CODE_RE: ëŒ€ë¬¸ì/ìˆ«ì 3~8 (ex. SAH001)
    MODEL_CODE_RE = re.compile(
        r"(?<![0-9A-Z])[0-9A-Z]{2,5}-[0-9A-Z]{2,10}(?![0-9A-Z])"
    )
    SIMPLE_CODE_RE = re.compile(
        r"(?<![0-9A-Z])[0-9A-Z]{3,8}(?![0-9A-Z])"
    )

    def __init__(
        self,
        embed_model: str = DEFAULT_EMBED_MODEL,
        output_dim: int = DEFAULT_OUTPUT_DIM,
        presearch_factor: int = DEFAULT_PRESEARCH_FACTOR,
    ) -> None:
        self.embed_model = embed_model
        self.output_dim = output_dim
        self.presearch_factor = presearch_factor

        # Lazy ì´ˆê¸°í™”ìš© ë‚´ë¶€ ìƒíƒœ
        self._client: Optional[genai.Client] = None
        self._index: Optional[faiss.IndexFlatIP] = None
        self._meta: List[Dict[str, Any]] = []

        # ì¸ë±ìŠ¤ + ë©”íƒ€ ë¡œë”©
        self._load_index_and_meta()

        # ğŸ”¹ ì œí’ˆ/ëª¨ë¸ ì½”ë“œ â†’ doc_id ë§¤í•‘ ì¸ë±ìŠ¤
        self._code_to_doc_ids: Dict[str, List[str]] = {}
        self._build_code_index()

    # ---------- ë‚´ë¶€ ì´ˆê¸°í™” ----------

    def _load_index_and_meta(self) -> None:
        """
        FAISS ì¸ë±ìŠ¤ì™€ vectors_meta.jsonl ì„ ë¡œë”©.
        """
        if not FAISS_INDEX_PATH.exists():
            raise FileNotFoundError(f"FAISS ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FAISS_INDEX_PATH}")
        if not VECTORS_META_PATH.exists():
            raise FileNotFoundError(f"vectors_meta.jsonl ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {VECTORS_META_PATH}")

        # 1) FAISS ì¸ë±ìŠ¤ ë¡œë”©
        self._index = faiss.read_index(str(FAISS_INDEX_PATH))

        # 2) ë©”íƒ€ ë¡œë”©
        meta_list: List[Dict[str, Any]] = []
        with VECTORS_META_PATH.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    meta = json.loads(line)
                except json.JSONDecodeError:
                    continue
                meta_list.append(meta)

        if len(meta_list) != self._index.ntotal:
            logger.warning(
                "[SEARCH] ë©”íƒ€ ë ˆì½”ë“œ ìˆ˜(%d)ì™€ ì¸ë±ìŠ¤ ë²¡í„° ìˆ˜(%d)ê°€ ë‹¤ë¦…ë‹ˆë‹¤.",
                len(meta_list),
                self._index.ntotal,
            )

        self._meta = meta_list

        logger.info(
            "[META] vectors_meta.jsonl ë¡œë”© ì™„ë£Œ: %dê°œ ë ˆì½”ë“œ (%s)",
            len(self._meta),
            VECTORS_META_PATH,
        )

    @property
    def client(self) -> genai.Client:
        """
        Gemini í´ë¼ì´ì–¸íŠ¸ lazy ì´ˆê¸°í™”.
        """
        if self._client is None:
            self._client = load_gemini_client()
            logger.info("Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
        return self._client

    @property
    def index(self) -> faiss.IndexFlatIP:
        if self._index is None:
            raise RuntimeError("FAISS ì¸ë±ìŠ¤ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return self._index

    @property
    def meta_list(self) -> List[Dict[str, Any]]:
        """
        vectors_meta.jsonl ì „ì²´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.
        """
        return self._meta

    # ---------- ì œí’ˆ/ëª¨ë¸ ì½”ë“œ ì¸ë±ì‹± ìœ í‹¸ ----------

    @staticmethod
    def _normalize_code(code: str) -> str:
        """
        ì½”ë“œ ë¬¸ìì—´ì„ ë¹„êµìš©ìœ¼ë¡œ ì •ê·œí™”:
        - ì•ë’¤ ê³µë°± ì œê±°
        - ëŒ€ë¬¸ì ë³€í™˜
        - [0-9A-Z-] ë§Œ ë‚¨ê¸°ê³  ì œê±°
        """
        c = code.strip().upper()
        c = re.sub(r"[^0-9A-Z-]", "", c)
        return c

    def _build_code_index(self) -> None:
        """
        vectors_meta.jsonl ì „ì²´ë¥¼ í›‘ì–´ì„œ
          "SBDH-T1000", "SAH001" ë“± â†’ [doc_id1, doc_id2, ...] ë§¤í•‘ì„ ë§Œë“ ë‹¤.
        """
        code_to_docs: Dict[str, List[str]] = defaultdict(list)

        for meta in self.meta_list:
            doc_id = str(meta.get("doc_id") or "").strip()
            if not doc_id:
                continue

            candidates: List[str] = []

            # 1) íŒŒì¼/ì¶œì²˜ ê´€ë ¨ í•„ë“œì—ì„œ ì½”ë“œ ì¶”ì¶œ
            for key in ("doc_id", "file_name", "file", "source"):
                v = str(meta.get(key) or "").upper()
                if not v:
                    continue

                # í•˜ì´í”ˆ í¬í•¨ ì½”ë“œ(SBDH-T1000 ë“±)
                for m in self.MODEL_CODE_RE.findall(v):
                    candidates.append(m)
                # ê°„ë‹¨ ì½”ë“œ(SAH001 ë“±)
                for m in self.SIMPLE_CODE_RE.findall(v):
                    candidates.append(m)

            # 2) í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ì—ì„œë„ ëª¨ë¸ ì½”ë“œê°€ ë…¸ì¶œë˜ëŠ” ê²½ìš°ê°€ ìˆì–´,
            #    í…ìŠ¤íŠ¸ ì•ìª½ 200ì ì •ë„ë§Œ í›‘ì–´ì„œ ì¶”ê°€ ì¶”ì¶œ
            text = str(meta.get("text") or "").upper()
            if text:
                head = text[:200]
                for m in self.MODEL_CODE_RE.findall(head):
                    candidates.append(m)

            # 3) ì¶”ì¶œëœ ì½”ë“œë“¤ì„ ì •ê·œí™” í›„ code_to_docs ì— ë“±ë¡
            for code in candidates:
                norm = self._normalize_code(code)
                if not norm:
                    continue
                docs = code_to_docs.setdefault(norm, [])
                if doc_id not in docs:
                    docs.append(doc_id)

        self._code_to_doc_ids = dict(code_to_docs)

        logger.info(
            "[CODE-INDEX] ì œí’ˆ/ëª¨ë¸ ì½”ë“œ ì¸ë±ì‹± ì™„ë£Œ: %dê°œ ì½”ë“œ ë§¤í•‘",
            len(self._code_to_doc_ids),
        )

    def extract_model_codes_from_query(self, query: str) -> List[str]:
        """
        ì§ˆì˜ë¬¸ì—ì„œ ì œí’ˆ/ëª¨ë¸ ì½”ë“œ íŒ¨í„´(SBDH-T1000, SAH001 ë“±)ì„ ì¶”ì¶œ.
        (ëŒ€ë¬¸ì/ìˆ«ì ê¸°ì¤€, í•˜ì´í”ˆ í¬í•¨)
        """
        q = query.upper()
        codes: List[str] = []

        # 1) ë¨¼ì € í•˜ì´í”ˆ í¬í•¨ ì½”ë“œ ìš°ì„  ì¶”ì¶œ (SBDH-T1000 ë“±)
        for m in self.MODEL_CODE_RE.findall(q):
            norm = self._normalize_code(m)
            if norm and norm not in codes:
                codes.append(norm)

        # 2) ê·¸ ë‹¤ìŒ ê°„ë‹¨ ì½”ë“œ(SAH001 ë“±)ë¥¼ ì¶”ê°€
        for m in self.SIMPLE_CODE_RE.findall(q):
            norm = self._normalize_code(m)
            if norm and norm not in codes:
                codes.append(norm)

        return codes

    def resolve_doc_ids_for_codes(self, codes: Sequence[str]) -> List[str]:
        """
        ì½”ë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„, ì½”ë“œ ì¸ë±ìŠ¤ì—ì„œ doc_id ë¦¬ìŠ¤íŠ¸ë¡œ í•´ì„.

        ğŸ”¸ ê¸°ë³¸ ë™ì‘:
            - ê° ì½”ë“œì— ë§¤í•‘ëœ doc_idë¥¼ ëª¨ë‘ ëª¨ì•„ ì¤‘ë³µ ì œê±° í›„ ë°˜í™˜

        ğŸ”¸ ì¶”ê°€ ê°œì„ :
            - 'SVC-WN2200MR', 'SVC', 'WN2200MR' ì²˜ëŸ¼ ì—¬ëŸ¬ ì½”ë“œê°€ ì„ì—¬ ìˆì„ ë•Œ
              "ìˆ«ìë¥¼ í¬í•¨í•œ, ë” ê¸´ ì½”ë“œ"ë¥¼ ìš°ì„ ìœ¼ë¡œ ì‚¬ìš©í•´ doc_idë¥¼ ì¢íŒë‹¤.
              (ê°€ì¥ êµ¬ì²´ì ì¸ ëª¨ë¸ ì½”ë“œì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë§Œ ìš°ì„  ì‚¬ìš©)
        """
        resolved_all: List[str] = []
        normalized_codes: List[str] = []

        # 1) ìš°ì„  ì „ì²´ ë§¤í•‘ ê²°ê³¼ë¥¼ ëª¨ì€ë‹¤.
        for code in codes:
            norm = self._normalize_code(code)
            if not norm:
                continue
            normalized_codes.append(norm)
            doc_ids = self._code_to_doc_ids.get(norm)
            if not doc_ids:
                continue
            for d in doc_ids:
                if d not in resolved_all:
                    resolved_all.append(d)

        if not resolved_all:
            return []

        # 2) "ìˆ«ìë¥¼ í¬í•¨í•œ ì½”ë“œ"ë§Œ ë½‘ì•„ ê¸¸ì´ ìˆœìœ¼ë¡œ ì •ë ¬ (ê¸´ ì½”ë“œê°€ ë” êµ¬ì²´ì )
        specific_codes = sorted(
            {c for c in normalized_codes if any(ch.isdigit() for ch in c)},
            key=len,
            reverse=True,
        )

        # 3) ê°€ì¥ êµ¬ì²´ì ì¸ ì½”ë“œë¶€í„°, í•´ë‹¹ ì½”ë“œ ë¬¸ìì—´ì´ doc_id ì•ˆì—
        #    (í•˜ì´í”ˆ ì œê±° í›„) ê·¸ëŒ€ë¡œ í¬í•¨ë˜ëŠ” doc_idë§Œ ë‚¨ê²¨ ë³¸ë‹¤.
        #
        #   ì˜ˆ) code="SVC-WN2200MR" â†’ "SVCWN2200MR"
        #       doc_id="SVC-WN2200MR_MANUAL" â†’ "SVCWN2200MRMANUAL"
        #       â†’ í¬í•¨ ê´€ê³„ê°€ ì„±ë¦½í•˜ë¯€ë¡œ, ì´ doc_idë¥¼ ìš°ì„  ì‚¬ìš©
        for sc in specific_codes:
            sc_norm = sc.replace("-", "")
            narrowed = [
                d
                for d in resolved_all
                if sc_norm in d.replace("-", "").upper()
            ]
            if narrowed:
                logger.info(
                    "[CODE-INDEX] ì½”ë“œ %s ê¸°ì¤€ìœ¼ë¡œ doc_idë¥¼ ì¢í˜”ìŠµë‹ˆë‹¤: %s",
                    sc,
                    ",".join(narrowed),
                )
                return narrowed

        # 4) íŠ¹ì • ì½”ë“œ ê¸°ì¤€ìœ¼ë¡œ ì¢í ìˆ˜ ì—†ìœ¼ë©´, ì „ì²´ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        return resolved_all

    # ---------- ì§ˆì˜ ì„ë² ë”© ----------

    def embed_query(self, query: str) -> np.ndarray:
        """
        ì‚¬ìš©ì ì§ˆì˜ë¥¼ text-embedding-004ë¡œ ì„ë² ë”©.
        """
        query = query.strip()
        if not query:
            raise ValueError("ë¹ˆ ì§ˆì˜ëŠ” ì„ë² ë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        resp = self.client.models.embed_content(
            model=self.embed_model,
            contents=[query],
            config=types.EmbedContentConfig(
                output_dimensionality=self.output_dim
            ),
        )
        vectors = extract_vectors_from_response(resp)
        if not vectors:
            raise RuntimeError("ì§ˆì˜ ì„ë² ë”© ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        vec = np.array(vectors[0], dtype="float32").reshape(1, -1)
        if vec.shape[1] != self.output_dim:
            logger.warning(
                "[SEARCH] ì§ˆì˜ ë²¡í„° ì°¨ì›(%d)ì´ ì„¤ì •ê°’(%d)ê³¼ ë‹¤ë¦…ë‹ˆë‹¤.",
                vec.shape[1],
                self.output_dim,
            )
        normalize_vector(vec)
        return vec

    # ---------- ê²€ìƒ‰ + ì¬ë­í‚¹ ----------

    def search(
        self,
        query: str,
        top_k: int = DEFAULT_TOP_K,
        chunk_type_filter: Optional[str] = None,   # "text" | "figure" | None
        doc_id_filter: Optional[List[str]] = None, # ["SAH001", ...] | None
    ) -> SearchResult:
        """
        1) (í•„ìš” ì‹œ) ì§ˆì˜ì—ì„œ ëª¨ë¸/ì œí’ˆ ì½”ë“œ ìë™ ì¶”ì¶œ â†’ doc_id_filter ìë™ ì„¤ì •
           - ì—¬ëŸ¬ ì½”ë“œê°€ ê°ì§€ë˜ë©´, ìˆ«ìë¥¼ í¬í•¨í•œ ë” êµ¬ì²´ì ì¸ ì½”ë“œì—
             ê°€ì¥ ì˜ ë§¤ì¹­ë˜ëŠ” doc_idë§Œ ìš°ì„  ì‚¬ìš©
        2) doc_id_filterê°€ ì„¤ì •ëœ ê²½ìš°:
             â†’ í•´ë‹¹ ë¬¸ì„œì˜ ë²¡í„°ë“¤ë§Œ ëŒ€ìƒìœ¼ë¡œ "ë¬¸ì„œ ë‚´ë¶€ ê²€ìƒ‰" ìˆ˜í–‰
        3) doc_id_filterê°€ ì—†ìœ¼ë©´:
             â†’ ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ FAISS ê²€ìƒ‰ (presearch_factor * top_k ë§Œí¼)
        4) í…ìŠ¤íŠ¸ ìš°ì„  + í‚¤ì›Œë“œ + (ì§ˆì˜ ì˜ë„ ê¸°ë°˜ ì„¹ì…˜/figure) ë¶€ìŠ¤íŒ…ìœ¼ë¡œ
           ì¬ë­í‚¹ í›„ top_k ê°œ ë°˜í™˜
        """
        if top_k <= 0:
            top_k = DEFAULT_TOP_K

        # 0) ğŸ”¹ doc_id_filter ìë™ ê°ì§€ (ìƒìœ„ ë ˆë²¨ì—ì„œ ì•ˆ ì¤¬ì„ ë•Œë§Œ)
        auto_doc_ids: List[str] = []
        if not doc_id_filter:
            codes = self.extract_model_codes_from_query(query)
            if codes:
                auto_doc_ids = self.resolve_doc_ids_for_codes(codes)
                if auto_doc_ids:
                    doc_id_filter = auto_doc_ids
                    logger.info(
                        "[CODE-INDEX] ì§ˆì˜ì—ì„œ ëª¨ë¸ ì½”ë“œ ê°ì§€ %s â†’ doc_id_filter ìë™ ì„¤ì •: %s",
                        ",".join(codes),
                        ",".join(auto_doc_ids),
                    )
                else:
                    logger.info(
                        "[CODE-INDEX] ì§ˆì˜ì—ì„œ ì½”ë“œ %s ê°ì§€ëì§€ë§Œ ë§¤í•‘ë˜ëŠ” doc_idë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                        ",".join(codes),
                    )
        else:
            # ìƒìœ„ ë ˆë²¨ì—ì„œ ì´ë¯¸ doc_id_filterë¥¼ ëª…ì‹œí•œ ê²½ìš°,
            # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©.
            codes = self.extract_model_codes_from_query(query)
            if codes:
                logger.info(
                    "[CODE-INDEX] ì§ˆì˜ì—ì„œ ëª¨ë¸ ì½”ë“œ %s ê°ì§€ë¨. "
                    "ê·¸ëŸ¬ë‚˜ ìƒìœ„ ë ˆë²¨ì—ì„œ doc_id_filterë¥¼ ì „ë‹¬í–ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                    ",".join(codes),
                )

        # í•„í„°/í‚¤ì›Œë“œ ì¤€ë¹„
        doc_id_set = set(doc_id_filter) if doc_id_filter else None
        chunk_type_filter_norm = (
            chunk_type_filter.lower() if chunk_type_filter else None
        )

        # 1) ì§ˆì˜ ì„ë² ë”© + í‚¤ì›Œë“œ ì¶”ì¶œ
        query_vec = self.embed_query(query)
        keywords = extract_keywords(query)
        q_flat = query_vec.astype("float32").reshape(-1)  # (D,)

        # ------------------------------------------------------------------
        # 1ë‹¨ê³„: doc_id_filterê°€ ì„¤ì •ëœ ê²½ìš° â†’ í•´ë‹¹ ë¬¸ì„œ ë²¡í„°ë“¤ë§Œ ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰
        # ------------------------------------------------------------------
        if doc_id_set:
            # ì´ ë¬¸ì„œë“¤ì— í•´ë‹¹í•˜ëŠ” row index ìˆ˜ì§‘
            row_indices: List[int] = []
            for idx, meta in enumerate(self.meta_list):
                doc_id = str(meta.get("doc_id") or "")
                if doc_id in doc_id_set:
                    row_indices.append(idx)

            if row_indices:
                logger.info(
                    "[SEARCH] doc_id_filter=%s ì ìš©: %dê°œ ë²¡í„°ì—ì„œë§Œ ê²€ìƒ‰ ìˆ˜í–‰",
                    ",".join(sorted(doc_id_set)),
                    len(row_indices),
                )

                candidates: List[RetrievedChunk] = []

                for row in row_indices:
                    # ê°œë³„ ë²¡í„° ë³µì› (IndexFlatIPëŠ” reconstruct ì§€ì›)
                    try:
                        vec = self.index.reconstruct(int(row))
                    except Exception as e:
                        logger.warning(
                            "[SEARCH] ì¸ë±ìŠ¤ reconstruct ì‹¤íŒ¨ (row=%d): %s",
                            row,
                            e,
                        )
                        continue

                    v = np.asarray(vec, dtype="float32").reshape(1, -1)
                    normalize_vector(v)
                    base_score = float(np.dot(q_flat, v.reshape(-1)))

                    meta = dict(self.meta_list[row])
                    doc_id = str(meta.get("doc_id") or "")
                    chunk_type = str(
                        meta.get("chunk_type") or meta.get("type", "") or ""
                    ).lower()
                    text = str(meta.get("text") or "")

                    # chunk íƒ€ì… í•„í„°
                    if chunk_type_filter_norm and chunk_type != chunk_type_filter_norm:
                        continue

                    uid = str(meta.get("uid") or meta.get("chunk_id") or f"{doc_id}:{row}")

                    # ì¬ë­í‚¹ ì ìˆ˜ ê³„ì‚°
                    final_score, type_boost, keyword_boost = compute_reranked_score(
                        base_score=base_score,
                        meta=meta,
                        keywords=keywords,
                    )

                    candidates.append(
                        RetrievedChunk(
                            uid=uid,
                            score=final_score,
                            raw_score=base_score,
                            doc_id=doc_id,
                            chunk_type=chunk_type or "text",
                            text=text,
                            meta=meta,
                        )
                    )

                # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ top_k ì„ íƒ
                candidates.sort(key=lambda c: c.score, reverse=True)
                top_chunks = candidates[:top_k]

                logger.info(
                    "[SEARCH] (ë¬¸ì„œ ë‚´ë¶€ ê²€ìƒ‰) í›„ë³´ %dê°œ â†’ ìµœì¢… ì»¨í…ìŠ¤íŠ¸ %dê°œ ë°˜í™˜ (ìš”ì²­ top_k=%d)",
                    len(candidates),
                    len(top_chunks),
                    top_k,
                )

                return SearchResult(
                    query=query,
                    top_k=top_k,
                    total_candidates=len(candidates),
                    chunks=top_chunks,
                )

            else:
                # doc_id_filterì—ëŠ” ê°’ì´ ìˆì§€ë§Œ ì‹¤ì œ ë©”íƒ€ì—ëŠ” í•´ë‹¹ doc_idê°€ ì—†ëŠ” ê²½ìš°
                logger.warning(
                    "[SEARCH] doc_id_filter=%s ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì „ì²´ ì½”í¼ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.",
                    ",".join(sorted(doc_id_set)),
                )
                # ì „ì²´ ê²€ìƒ‰ ê²½ë¡œë¡œ ë„˜ì–´ê°€ë„ë¡ doc_id_set ì´ˆê¸°í™”
                doc_id_set = None

        # ------------------------------------------------------------------
        # 2ë‹¨ê³„: doc_id_filterê°€ ì—†ê±°ë‚˜, ë§¤ì¹­ ì‹¤íŒ¨ â†’ ê¸°ì¡´ ì „ì²´ ì½”í¼ìŠ¤ ê²€ìƒ‰
        # ------------------------------------------------------------------
        pre_k = min(self.index.ntotal, top_k * self.presearch_factor)
        doc_filter_log = "ì „ì²´" if not doc_id_set else ",".join(sorted(doc_id_set))

        logger.info(
            "[SEARCH] (ì „ì²´ ê²€ìƒ‰) ì§ˆì˜ ì„ë² ë”© ì™„ë£Œ. top_k=%d (presearch=%d), "
            "chunk_type_filter=%s, doc_id_filter=%s",
            top_k,
            pre_k,
            chunk_type_filter or "None",
            doc_filter_log,
        )

        scores, indices = self.index.search(query_vec, pre_k)

        candidates: List[RetrievedChunk] = []
        for rank_idx in range(pre_k):
            row = int(indices[0, rank_idx])
            base_score = float(scores[0, rank_idx])

            if row < 0 or row >= len(self.meta_list):
                continue

            meta = dict(self.meta_list[row])
            doc_id = str(meta.get("doc_id") or "")
            chunk_type = str(
                meta.get("chunk_type") or meta.get("type", "") or ""
            ).lower()
            text = str(meta.get("text") or "")

            # í•„í„°ë§
            if doc_id_set and doc_id not in doc_id_set:
                continue
            if chunk_type_filter_norm and chunk_type != chunk_type_filter_norm:
                continue

            uid = str(meta.get("uid") or meta.get("chunk_id") or f"{doc_id}:{row}")

            # ì¬ë­í‚¹ ì ìˆ˜ ê³„ì‚°
            final_score, type_boost, keyword_boost = compute_reranked_score(
                base_score=base_score,
                meta=meta,
                keywords=keywords,
            )

            candidates.append(
                RetrievedChunk(
                    uid=uid,
                    score=final_score,
                    raw_score=base_score,
                    doc_id=doc_id,
                    chunk_type=chunk_type or "text",
                    text=text,
                    meta=meta,
                )
            )

        # ì¬ë­í‚¹ ê²°ê³¼ ì •ë ¬
        candidates.sort(key=lambda c: c.score, reverse=True)
        top_chunks = candidates[:top_k]

        logger.info(
            "[SEARCH] ì¬ë­í‚¹ ì™„ë£Œ. í›„ë³´ %dê°œ â†’ ìµœì¢… ì»¨í…ìŠ¤íŠ¸ %dê°œ ë°˜í™˜ (ìš”ì²­ top_k=%d)",
            len(candidates),
            len(top_chunks),
            top_k,
        )

        return SearchResult(
            query=query,
            top_k=top_k,
            total_candidates=len(candidates),
            chunks=top_chunks,
        )


# ----------------------------- ìŠ¤í¬ë¦½íŠ¸ë¡œ ì§ì ‘ ì‹¤í–‰ ì‹œ -----------------------------


def _interactive_cli() -> None:
    """
    ê°„ë‹¨í•œ CLI í…ŒìŠ¤íŠ¸ìš©:
        (.venv) > python -m src.rag_search_gemini
    """
    configure_logging()
    searcher = RagSearcher()

    print("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (rag_search_gemini) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ìƒìœ„ ê²°ê³¼ë“¤ì˜ doc_id / íƒ€ì… / ì ìˆ˜ ë“±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.")
    print("ëª¨ë¸ ì½”ë“œê°€ ì„ì¸ ì§ˆì˜(SVC-WN2200MR, SBDH-T1000 ë“±)ë„ í…ŒìŠ¤íŠ¸í•´ ë³´ì„¸ìš”.")
    print("ì¢…ë£Œ: ë¹ˆ ì¤„ + Enter ë˜ëŠ” Ctrl+C\n")

    while True:
        try:
            q = input("ì§ˆë¬¸: ").strip()
        except (EOFError, KeyboardInterrupt):
            break

        if not q:
            break

        result = searcher.search(q, top_k=5)
        print(f"\n[ê²€ìƒ‰ ê²°ê³¼] top_k={result.top_k}, í›„ë³´={result.total_candidates}ê°œ\n")

        for i, ch in enumerate(result.chunks, start=1):
            ct = ch.chunk_type.upper()
            doc = ch.doc_id
            score = ch.score
            raw = ch.raw_score
            sec = ch.meta.get("section_title") or ch.meta.get("category") or ""
            page = ch.meta.get("page") or ch.meta.get("page_start")
            page_info = f"p.{page}" if page is not None else "p.?"
            print(
                f"[{i}] {ct} | {doc} ({page_info}) "
                f"| score={score:.4f} (raw={raw:.4f}) | {sec}"
            )
        print()

    print("ì¢…ë£Œí•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    _interactive_cli()
